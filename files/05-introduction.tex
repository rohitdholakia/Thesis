%!TEX root=/home/ska124/Dropbox/Thesis/thes-full.tex
%% Copyright 1998 Pepe Kubon
%%
%% `05-introduction.tex' --- 1st chapter for thes-full.tex, thes-short-tex from
%%                the `csthesis' bundle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%       Chapter 1 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{sec:introduction}

\section{Why study Low-Resource?}
\label{sec:low_resource}
Statistical Machine Translation(SMT) uses data-driven models to translate sentences in a source language to a given target language. A phrase-based SMT system has a generic pipeline that looks as described in Algorithm~\ref{algo:pbsmt}. 

\begin{algorithm}
\small
%\centering
\caption{Building a phrase-based system}
\label{algo:pbsmt}
\textbf{Input}: Parallel corpus between \emph{s} and \emph{t} \\
\textbf{Output}: A translation model ``tm'' 
\begin{algorithmic}[l]
	%\STATE{\textbf{Clean:}Pre-process both sides of the corpus} \label{aline:preprocess}
	\STATE{\textbf{Alignments: }Learn bi-directional alignments} \label{aline:alignments}
	\STATE{\textbf{Extraction: }Extract phrase pairs from alignments and compute likelihoods for each translation pair} \label{aline:scores}
	\STATE{\textbf{Tuning: }Set weights for features by maximizing BLEU score on a development set using MERT} \label{aline:MERT}
\end{algorithmic}
\end{algorithm}

Each step of Algorithm~\ref{algo:pbsmt} outlined above raises questions when faced with a low-resource language pair. Lets consider each step and discuss the problems that come up. Given parallel data, the goal of the alignment models~\cite{Brown:1993,Vogel:1996} is to learn which word in source language \emph{s} translates to target language \emph{t} and assign a likelihood to the pair. Facing corpus of small sizes, the models place higher likelihood on pairs seen fewer number of times. The phrase extraction step(Line~\ref{aline:scores}) looks at alignments learnt from Line~\ref{aline:alignments} in both directions and determines which phrases can translate from one language to another. At the end of this step, we have a phrase table which has rules of the following form : 

\begin{table*}
\small
\small
\centering
\begin{tabular}{p{0.3\linewidth}p{0.2\linewidth}p{0.4\linewidth}}
\toprule
src & tgt & features \\
\toprule
! la situacion de haiti & concerned about the situation in haiti & 0.5 8.16237e-09 1 0.000483004 2.718 \\
\bottomrule
\end{tabular}
\end{table*}

The above rule states that the source phrase ``! la situacion de haiti , '' translate to the target phrase ``concerned about the situation in haiti ,'' with the feature values shown on the right. 

\begin{table*}
	\small
	\small
	%\centering
	\begin{tabular}{p{0.3\linewidth}p{0.6\linewidth}}
	\toprule
	Feature &  Explanation \\
	\toprule
	$p_{w}(f \mid e)$ & probability of seeing phrase ``f'' given ``e'' \\
	$p_{lex}(f \mid e)$ & lexical probability of seeing phrase ``f'' given ``e'' \\
	$p_{w}(e \mid f)$ & probability of seeing phrase ``e'' given ``f'' \\
	$p_{lex}(e \mid f)$ & lexical probability of seeing phrase ``e'' given ``f'' \\
   	phrase penalty & a constant value penalizing distortion \\
	\bottomrule
	\end{tabular}
	\caption{Features of the phrase pairs, where ``f'' is foreign/source \& ``e'' is target/english}
	\label{table:features}
\end{table*}

The 5 features are mentioned in Table~\ref{table:features}. The two $p_{w}$ are the phrasal features, features that determine the likelihood of the source phrase translating to target and vice-versa. The phrasal translation likelihood is computed by using relative frequencies, as shown in equation~\eqref{eq:trans}.

\begin{equation} \label{eq:trans}
	p_{w}(f \mid e) = \frac{c(f, e)}{\mathlarger{\sum\limits_{\grave{f}}}c(\grave{f}, e)}
\end{equation}

The counts referred to in equation~\eqref{eq:trans} are obtained from the alignments. Note that the alignment models that were learnt on the smaller corpus will cause some propagation of errors in the phrasal probabilities. 

 The lexical features~\cite{Koehn:03} are actually computed as shown in equation~\eqref{eq:lex} : 

\begin{equation} \label{eq:lex}
	p_{lex}(f \mid e, a) = \prod\limits_{i=1}^{n} \frac{1}{\{j | (i,j) \in a\}}
	\mathlarger{\sum\limits_{\forall (i,j) \in a}} w(f_{i} \mid e_{j})
\end{equation}


The intuition behind having a pair of lexical features is to reward syntactic phrases while penalizing spurious one. As shown in equation~\eqref{eq:lex}, the lexical probability is the product of the lexical alignment probabilities of the constituent words in the phrase table. Hence, if a longer source phrase aligns to an equally long target phrase, it can be penalized if the individual words are not aligned. 

Phrase-based SMT has been used with great success before in the literature. But, as described above, the approach is very data-driven and it is not clear how to negate the effectiveness of data and achieve fluent translations. 

The approach of triangulation~\cite{Cohn:07,Utiyama:07,Nakov:12} scales the feature values we compute using the smaller corpora using a translation model learnt from a much larger and cleaner corpus. A pattern in the previous studies is that a large translation model is used to augment an already large translation model. In a scenario like that, only the rich become richer.  

\section{Dissertation Outline}
\label{sec:outline}
Chapter~\ref{chapter:reality} describes the four low-resource languages we study followed by the models and results. \alert{add the chapter about Europarl}.

\section{Contributions}
\label{sec:summary}
We conduct the first in-depth study of triangulation, the first using four real low-resource languages with realistic data settings. As part of the dissertation, we also build the first translation systems for three of the four languages. Our best Haitian-Creole system outperform the best system from the Sixth Workshop on Machine Translation, 2011. 


\section{Setup}
\label{sec:setup}


Moses~\cite{Koehn:07} was used for all the experiments. To build our baseline systems, we followed the standard set of steps: generated bi-directional alignments using GIZA++ ~\cite{OchNey:03}, followed by phrase extraction using the --grow-diag-final-and heuristic. The decoder parameters were optimized using Minimum Error Rate Training ~\cite{Och:03} by minimizing BLEU loss on a development set. All scores reported are case-insensitive BLEU~\cite{Papineni:02}. All language models were generated using SRILM~\cite{Stolcke:02}.~\cite{Ken:11} was used for language model scoring when decoding.