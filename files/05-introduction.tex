%!TEX root=/home/ska124/Dropbox/Thesis/thes-full.tex
%% Copyright 1998 Pepe Kubon
%%
%% `05-introduction.tex' --- 1st chapter for thes-full.tex, thes-short-tex from
%%                the `csthesis' bundle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%       Chapter 1 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{sec:introduction}

\section{Why study Low-Resource?}
\label{sec:low_resource}
Statistical Machine Translation(SMT) uses data-driven models to translate sentences in a source language to a given target language. A phrase-based SMT system has a generic pipeline that looks as described in Algorithm~\ref{algo:pbsmt}. 

\begin{algorithm}
\small
%\centering
\caption{Building a phrase-based system}
\label{algo:pbsmt}
\textbf{Input}: Parallel corpus between \emph{s} and \emph{t} \\
\textbf{Output}: A translation model ``tm'' 
\begin{algorithmic}[l]
	%\STATE{\textbf{Clean:}Pre-process both sides of the corpus} \label{aline:preprocess}
	\STATE{\textbf{Alignments: }Learn bi-directional alignments} \label{aline:alignments}
	\STATE{\textbf{Extraction: }Extract phrase pairs from alignments and compute likelihoods for each translation pair} \label{aline:scores}
	\STATE{\textbf{Tuning: }Set weights for features by maximizing BLEU score on a development set using MERT} \label{aline:MERT}
\end{algorithmic}
\end{algorithm}

Each step of Algorithm~\ref{algo:pbsmt} outlined above raises questions when faced with a low-resource language pair. Given parallel data, the goal of the alignment models is to learn which word in source language \emph{s} translates to target language \emph{t} and assign a likelihood to the pair. When faced with a little data, we place confident on translation pairs based on having seen them a few times. 






\section{Dissertation Outline}
\label{sec:outline}


\section{Contributions}
\label{sec:summary}
We conduct the first in-depth study of triangulation, the first using four real low-resource languages with realistic data settings. As part of the dissertation, we also build the first translation systems for three of the four languages. Our best Haitian-Creole system outperform the best system from the Sixth Workshop on Machine Translation, 2011. 


\section{Setup}
\label{sec:setup}


Moses~\cite{Koehn:07} was used for all the experiments. To build our baseline systems, we followed the standard set of steps: generated bi-directional alignments using GIZA++ ~\cite{OchNey:03}, followed by phrase extraction using the --grow-diag-final-and heuristic. The decoder parameters were optimized using Minimum Error Rate Training ~\cite{Och:03} by minimizing BLEU loss on a development set. All scores reported are case-insensitive BLEU~\cite{Papineni:02}. All language models were generated using SRILM~\cite{Stolcke:02}.~\cite{Ken:11} was used for language model scoring when decoding.