%!TEX root=/home/ska124/Dropbox/Thesis/thes-full.tex
%% Copyright 1998 Pepe Kubon
%%
%% `05-introduction.tex' --- 1st chapter for thes-full.tex, thes-short-tex from
%%                the `csthesis' bundle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%       Chapter 1 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\label{sec:introduction}

\section{Why study Low-Resource?}
\label{sec:low_resource}
Statistical Machine Translation(SMT) uses data-driven models to translate sentences in a source language to a given target language. A phrase-based SMT system has a generic pipeline that looks as described in Algorithm~\ref{algo:pbsmt}. 

\begin{algorithm}
\small
%\centering
\caption{Building a phrase-based system}
\label{algo:pbsmt}
\textbf{Input}: Parallel corpus between \emph{s} and \emph{t} \\
\textbf{Output}: A translation model ``tm'' 
\begin{algorithmic}[l]
	%\STATE{\textbf{Clean:}Pre-process both sides of the corpus} \label{aline:preprocess}
	\STATE{\textbf{Alignments: }Learn bi-directional alignments} \label{aline:alignments}
	\STATE{\textbf{Extraction: }Extract phrase pairs from alignments and compute likelihoods for each translation pair} \label{aline:scores}
	\STATE{\textbf{Tuning: }Set weights for features by maximizing BLEU score on a development set using MERT} \label{aline:MERT}
\end{algorithmic}
\end{algorithm}

The reason its called phrase-based SMT is because the base unit of translation are \emph{phrases}. The phrases need not be linguistically motivated. Phrases in this context means a group of words. In this dissertation, we only use phrase-based SMT for all the experiments but there are others in the literature. 

Each step of Algorithm~\ref{algo:pbsmt} outlined above raises questions when faced with a low-resource language pair. Low-resource languages are those with insufficient resources to use for Machine Translation into and/or from the language. To provide perspective, French has a corpus with $10^9$ parallel sentences with English. On the other hand, the language with the highest amount of data in this dissertation is Haitian Kreyol, with 121K sentences. Out of those 121K, only 16\% are from the target domain, the sentences are noisy and with punctuation and spelling mistakes.


Lets consider each step and discuss the problems that come up. Given parallel data, the goal of the alignment models~\cite{Brown:1993,Vogel:1996} is to learn which word in source language \emph{s} translates to target language \emph{t} and assign a likelihood to the pair. The advanced alignment models use initial alignments from Model 1. Model 1 starts with uniform initialization and uses Expectation Maximization~\cite{Dempster:77} to converge. Facing corpus of a small size, the alignment models will end up making inferences that are not always true. They will place higher likelihood on pairs seen fewer number of times. The phrase extraction step(Line~\ref{aline:scores}) looks at alignments learnt from Line~\ref{aline:alignments} in both directions and determines which phrases can translate from one language to another using the intersection of the alignments. At the end of this step, we have a phrase table which has rules of the following form : 

\begin{table*}
\small
\small
\centering
\begin{tabular}{p{0.3\linewidth}p{0.2\linewidth}p{0.4\linewidth}}
\toprule
src & tgt & features \\
\toprule
! la situacion de haiti & concerned about the situation in haiti & 0.5 8.16237e-09 1 0.000483004 2.718 \\
\bottomrule
\end{tabular}
\end{table*}

The above rule states that the source phrase ``! la situacion de haiti , '' translate to the target phrase ``concerned about the situation in haiti ,'' with the feature values shown on the right. 

\begin{table*}
	\small
	\small
	%\centering
	\begin{tabular}{p{0.3\linewidth}p{0.6\linewidth}}
	\toprule
	Feature &  Explanation \\
	\toprule
	$p_{w}(f \mid e)$ & probability of seeing phrase ``f'' given ``e'' \\
	$p_{lex}(f \mid e)$ & lexical probability of seeing phrase ``f'' given ``e'' \\
	$p_{w}(e \mid f)$ & probability of seeing phrase ``e'' given ``f'' \\
	$p_{lex}(e \mid f)$ & lexical probability of seeing phrase ``e'' given ``f'' \\
   	phrase penalty & a constant value penalizing distortion \\
	\bottomrule
	\end{tabular}
	\caption{Features of the phrase pairs, where ``f'' is foreign/source \& ``e'' is target/english}
	\label{table:features}
\end{table*}

The 5 features are mentioned in Table~\ref{table:features}. The two $p_{w}$ are the phrasal features, features that determine the likelihood of the source phrase translating to target and vice-versa. The phrasal translation likelihood is computed by using relative frequencies, as shown in equation~\eqref{eq:trans}.

\begin{equation} \label{eq:trans}
	p_{w}(f \mid e) = \frac{c(f, e)}{\mathlarger{\sum\limits_{\grave{f}}}c(\grave{f}, e)}
\end{equation}

The counts referred to in equation~\eqref{eq:trans} are obtained from the alignments. Note that the alignment models that were learnt on the smaller corpus will cause some propagation of errors in the phrasal probabilities. 

 The lexical features~\cite{Koehn:03} are actually computed as shown in equation~\eqref{eq:lex} : 

\begin{equation} \label{eq:lex}
	p_{lex}(f \mid e, a) = \prod\limits_{i=1}^{n} \frac{1}{\{j | (i,j) \in a\}}
	\mathlarger{\sum\limits_{\forall (i,j) \in a}} w(f_{i} \mid e_{j})
\end{equation}


The intuition behind having a pair of lexical features is to reward syntactic phrases while penalizing spurious one. As shown in equation~\eqref{eq:lex}, the lexical probability is the product of the lexical alignment probabilities of the constituent words in the phrase table. Hence, if a longer source phrase aligns to an equally long target phrase, it can be penalized if the individual words are not aligned. 

\alert{Add stuff about decoding and BLEU scores}

Phrase-based SMT has been used with great success before in the literature. But, as described above, the approach is very data-driven and it is not clear how to achieve fluent translations with only a little parallel data. 


The approach of triangulation~\cite{Cohn:07,Utiyama:07,Nakov:12} aims to add translations for new source phrases while also improving translations for existing source phrases. New source phrases can be added if one has a rich source pivot corpora that leads to a new target phrase in the pivot target corpus. In a low resource scenario, its important to achieve both aims with triangulation. Owing to less training data, the direct system has several out-of-vocabulary(OOV) words. We aim to reduce the number by using triangulation. But, in case of Haitian Kreyol and Malagasy, where our source pivot corpus is completely disjoint and out-of-domain with the direct and pivot target system, we also aim to improve the translations of existing phrases. As we will show in section ~\ref{sec:results}, we do improve translations for both.  


\section{Dissertation Outline}
\label{sec:outline}
Chapter~\ref{chapter:reality} describes the four low-resource languages we study followed by the models and results. \alert{add the chapter about Europarl}.

\section{Contributions}
\label{sec:summary}
We conduct the first in-depth study of triangulation, the first using four real low-resource languages with realistic data settings. As part of the dissertation, we also build the first translation systems for three of the four languages. Our best Haitian-Creole system outperform the best system from the Sixth Workshop on Machine Translation, 2011. 


\section{Setup}
\label{sec:setup}


Moses~\cite{Koehn:07} was used for all the experiments. To build our baseline systems, we followed the standard set of steps: generated bi-directional alignments using GIZA++ ~\cite{OchNey:03}, followed by phrase extraction using the --grow-diag-final-and heuristic. The decoder parameters were optimized using Minimum Error Rate Training ~\cite{Och:03} by minimizing BLEU loss on a development set. All scores reported are case-insensitive BLEU~\cite{Papineni:02}. All language models were generated using SRILM~\cite{Stolcke:02}.~\cite{Ken:11} was used for language model scoring when decoding.