\chapter{Triangulation}
\label{chap:triangulation}


\section{What is Triangulation?}
\label{sec:triangulation}

Consider a source language, \emph{s}, a target language, \emph{t}, and a pivot language \emph{i}. You have a little parallel data between \emph{s} and \emph{t} and believe that triangulation will increase the quality of translations between \emph{s} and \emph{t}. What steps one would follow to get the desired result ?

\begin{algorithm}
\caption{Vanilla Triangulation}
\label{algo:vanilla_triangulation}

\textbf{Input:} phrase table between \emph{s} and \emph{i}, p$_{s-i}$, \\
 phrase table between \emph{p} and \emph{t}, p$_{i-t}$,  \\
 \emph{n} for selecting top-n phrase pairs \\
\textbf{Output:} p$_{trian}$
\begin{algorithmic}[l]
\FORALL{(src, pivot) in top-\emph{n} p$_{s-i}$} \label{aline:common}
		\IF{pivot phrase in p$_{i-t}$} 

        \FOR{all (pivot, tgt) pairs in p$_{i-t}$}
        \STATE{compute feature values for (s, t)} \label{aline:computation}
        \ENDFOR
        \STATE{select top-\emph{n} src-target pair, add to p$_{trian}$} \label{aline:topn}
        \ENDIF
        \ENDFOR


%\ENDFOR
\end{algorithmic}

\end{algorithm}

The algorithm for triangulation is described in algorithm~\ref{algo:vanilla_triangulation}. Having obtained new source target pairs by using the common pivot phrases in line~\ref{aline:common}, we proceed to compute the feature values for the new phrase pairs(line~\ref{aline:computation}). To minimize the noise, we only select the top-\emph{n} translations for any given source phrase(line~\ref{aline:topn}).  

Having obtained a new source target pair, how best to compute the feature values ? For source phrase src, target phrase tgt and pivot phrase pvt, we can compute the feature values like in~\cite{Utiyama:07} using the following equations : 

 		\begin{equation*}
                 p_{lex}(tgt \mid src) = \sum_{pvt} p_{lex}(tgt \mid pvt) p_{lex}(pvt \mid src)
        \end{equation*}

        \begin{equation*}
        	p_{lex}(src \mid tgt) = \sum_{pvt} p_{lex}(src \mid pvt) p_{lex}(pvt \mid tgt)
        \end{equation*}

         \begin{equation*}
        	p_w(tgt \mid src) = \sum_{pvt} p_w(tgt \mid pvt) p_w(pvt \mid src)
        \end{equation*}

        \begin{equation*}
        	p_w(src \mid tgt) = \sum_{pvt} p_w(src \mid pvt) p_w(pvt \mid tgt)
        \end{equation*}

        For all the feature values, we multiply the corresponding values for source pivot and pivot target entries and marginalize over the pivot phrase. Note that we are making an independence assumption shown in equation below. 

         \begin{eqnarray*} \label{eq:independence}
                p(tgt \mid src)&=&\sum_{pvt}{p(tgt, pvt \mid src)}\\
                &=& \sum_{pvt}{p(tgt \mid pvt, src)\,p(pvt \mid src)}\\
                &\approx& \sum_{pvt}{p(tgt \mid pvt)\,p(pvt \mid src)}
        \end{eqnarray*}


\alert{keep an elaborate example here}



\section{Related Work}
 Consider a source language \emph{s}, pivot language \emph{p} and target language \emph{t}. When using the \emph{cascading} approach, we build two systems, between \emph{s} and \emph{p} and between \emph{p} and \emph{t}. Given a test set in \emph{s}, it is first translated to \emph{p} and those output translations are then translated into the target language \emph{t}, making decoding twice as expensive as well. We do not report our results on using cascading for various reasons. Firstly, translating the output of a source pivot system trained and tuned on little data will lead to propogation of errors. Secondly, we will need three development sets, one for each system. Finding standard development sets for low-resource languages is unlikely. Finally, it has been shown before that cascading does not give the most fluent translations.~\cite{Utiyama:07} compared pivot-based triangulation with cascading using all of multi-parallel europarl, observing that pivot-based methods outperformed cascading.

 The second approach is the pivot-based approach where a triangulated phrase table is generated between the source and target, by using the common pivot phrases between the source pivot and pivot target tables~\cite{Utiyama:07,Cohn:07}.~\cite{Utiyama:07} observed that the triangulated table was able to achieve comparable BLEU scores to the direct system for French, German and Spanish. This could be owing to the fact that the data comprised multi-parallel 560K sentences.~\cite{Cohn:07} observe that multiple pivot languages lead to more fluent translations compared to one pivot language. Multiple pivot language lead to multiple alternative translations, thus, increasing phrase coverage and rewarding the more appropriate translations and reducing out-of-vocabulary words further. They also propose a systematic way of combining the triangulated translation model with the direct model using linear interpolation and log-linear interpolation, although they assume a uniform weight for both the models. To ``simulate'' a low-resource scenario, the top 10K multi-parallel sentences are considered for source pivot, pivot target and source target systems. As we will observe later, real low-resource scenarios are significantly different from how it was simulated in~\cite{Cohn:07}.~\cite{Nakov:12} propose a language-independent approach to improving translation for low-resource languages, but the approach assumes the presence of a resource-rich language that bears similarity to the low-resource language, the similarities helping in creating a large triangulated phrase table. In~\cite{Nakovemnlp:12}, the resource-rich language is adapted to be more like the resource-poor one. Notice that this also assumes both are very similar. Results are reported using both Malay-Indonesian and Bulgarian-Macedonian, the third language being English in both cases.~\cite{Gispert:06} translate Catalan to Spanish via English by using news corpora on both source pivot and pivot target side.~\cite{Huck:12} report on BLEU score improvements by using $10^9$ parallel sentence between German and French.

 
 A common thread that binds the previous work using the approach of Triangulation is the usage of resource-rich languages. The fundamental reason behind the effectiveness of Triangulation is the reduction in the number of OOVs when using the pivot language(s). This can be observed in various forms. If the source and pivot language have a healthy vocabulary overlap, the SMT system between source-pivot is large, thus, improving translations. This factor also helps when the amount of parallel text between source-pivot is relatively low, e.g, Indonesian-English.  All the europarl languages are based on parlimentary proceedings and have minimal noise. Hence, the improvements using triangulation over the direct systems cannot be generalized for systems for low-resource languages. All the papers that use triangulation in machine translation cite either~\cite{Utiyama:07} or~\cite{Cohn:07}, both published in 2007 (and sometimes cite both of them but use either one model or the other). However, these two papers introduce triangulation for phrase-based SMT in very different ways and their models are different from each other. ``Simulating'' low-resource scenarios is ineffective in various ways. Firstly, real low-resource languages are noisy, not perfectly sentence aligned, and do not have a lot of data in the target domain. Secondly, triangulation is highly dependent on how good is the source pivot bitext. If the size of source pivot bitext is comparable to the source target, and/or is in the same domain, this increases bias in triangulation by introducting several common phrases, and, this is also not seen in a real low-resource setting.

 \alert{Add stuff about domain adaptation and all that}



 To our knowledge, before this dissertation, there has been no in-depth study of the different choices in building an SMT system using triangulation. Another limitation of the original work in triangulation~\cite{Utiyama:07,Cohn:07} is the unrealistic use of languages with abundant parallel data to simulate low-resource languages. Subsequent work~\cite{Nakov:12,Nakovemnlp:12,Gispert:06,Huck:12} has also assumed that parallel data in pivot languages can be found in the same domain as the original resource-poor language pair. This kind of domain similarity is not easy to find for realistic low-resource settings.

 \section{Europarl}
 \label{sec:europarl}
Europarl is short for European Parliament and refers to the multi-parallel corpora generated by translating the proceedings of European parliament into several languages. Version 7 of Europarl now has 20 languages, from French to Estonian and Finnish. Release of the Europarl corpus led to a surge in research into more and more data-driven methods to enable Statistical Machine Translation. The results were easily reproducible and the data is very clean and sentence-aligned. Each language has development and test sets which are used to report and reproduce results. 

What does multi-parallel imply? Consider english as the common target language. A multi-parallel corpora between 20 European languages and English comprises sentences in 20 european languages which translate to the same english sentence. In Table~\ref{table:mparallel}, an english sentence and its corresponding translations in French, Spanish and German are shown. These are the 10th sentence in each of the files. When using triangulation for a multi-parallel corpora like Europarl, one is effectively tracing different steps to the same target. This helps in expanding the resulting phrase table due to many common phrases. 

\begin{table*}
	\small
	\centering
	\small
	\input{files/Tables/example-europarl.tex}
	\caption{Multi-parallel example: en = English, de = German, fr = French, es = Spanish}
	\label{table:mparallel}
\end{table*}


\section{Models}
\label{sec:models}

\subsection{Top-\emph{n} filtering}
\label{subsec:topn}
 The size of the triangulated phrase table is controlled by the number of translations \emph{n} considered for a given source phrase. Consider a source phrase \emph{p$_s$} that translates to \emph{p$_p$} in the pivot language. The phrase \emph{p$_p$} has 1293 translations in the pivot target table. Considering all the 1293 translations will result in 1293 translations for the phrase \emph{p$_s$} via one pivot phrase. It is reasonable to expect the phrase \emph{p$_s$} to have multiple pivot translations, all having a higher number of translations in pivot target. Considering all translations is not recommended for several reasons. Firstly, this will lead to a very large phrase table. Table~\ref{table:allrules} shows the number of rules we can end up with if we consider all possible paths to a target phrase. To put it in perspective, the direct table for Mawukakan and Maninkakan have 51K and 60K phrase pairs respectively. Secondly, alongwith valid translations, triangulation also adds some noise to the translations by considering several translations of the common pivot phrase. Considering all translations would add even more noise to our triangulated phrase table. Having said that, when one has only 5000 parallel sentences for the direct system, how large a value of \emph{n} is enough ? In section~\ref{sec:results}, we report results on n = \{20, 40, 60, 80, 100\}. Figure~\ref{fig:multiplication_factors} shows the increase in the size of the triangulated phrase table when compared to the direct phrase table for all the four languages. For all the four languages, the increase in size of the triangulated table is linear. The difference between n = 20 and say, n = 80, is not surprisingly large. For Mawukakan and Maninkakan, we observe that the initial multiplication factor is much larger than the other two languages, despite having a tiny source pivot corpora.

        \begin{table}
                \footnotesize
                \small
                \centering

                \begin{tabular}{p{0.3\linewidth}p{0.3\linewidth}p{0.3\linewidth}}
                \toprule
                language & common pivot & Rules \\
                \toprule
                maninkakan & 6289 & 106.7M \\
                mawukakan & 8128 & 151.6M \\
                \bottomrule
                \end{tabular}
                \caption{Number of rules if all possible paths are considered}
                \label{table:allrules}
        \end{table}


\subsection{Connectivity features}
\label{subsec:connectivity}
        The phrase pairs in the triangulated phrase table are contingent upon the common pivot phrases. As a result, we can have phrase pairs that map ``!'' to a target phrase ``and making the soup thick !'' in Haitian Kreyol to English triangulated phrase table. Computing feature values using equations from section~\ref{sec:topn}, it is reasonable to assume that spurious phrase pairs like above can get a high enough feature value to end up as a translation during decoding. To reward phrase pairs that have more alignment links between and to penalize pairs that don't, we add two connectivity features to the phrase table, as used in ~\cite{Ahmed:13}.

        For a source phrase \emph{p$_s$}, target phrase \emph{p$_t$}, and with the number of alignment links between them \emph{N}, the strength will be calculated as follows :

        \begin{equation*}
                source_{strength} = \frac{\mathrm{N}}{\mathrm{S}}
        \end{equation*}

        \begin{equation*}
                target_{strength} = \frac{\mathrm{N}}{\mathrm{T}}
        \end{equation*}

        where S is the length of the source phrase \emph{p$_s$} and T is the length of the target phrase \emph{p$_t$}. To compute the connectivity strength feature, the alignments in the source pivot phrase pair are intersected with the pivot target phrase pair. If the resulting alignment has a higher strength, it implies that a majority of the source words do have an alignment with the target.


\subsection{IBM Model1 Alignment}
\label{subsec:model1}

        In section~\ref{sec:topn}, we computed the lexical scores by multiplying the component scores and marginalizing over the pivot phrase. The component lexical scores are a measure of the word-to-word alignment~\cite{Koehn:03} and by multiplying them, the final lexical scores are implying some strength-of-tie for each pair in the source target translation. But, as was discussed in~\ref{sec:topn}, using triangulation adds some noise to the translation model by proposing spurious translations.

        An alternative way to compute the lexical score is to use a Model 1~\cite{Brown:1993} score between the phrase pairs in the triangulated table. Treating the triangulated phrase table as a parallel corpus, we learn the model 1 alignment scores in both directions using 5 iterations of the EM algorithm~\cite{Dempster:77}. Given a foreign sentence f = f$_{1}$, . . . ., f$_{m}$, english sentence e = e$_{1}$, . . . , e$_{l}$, the model 1 score between the sentences is calculated as follows:

                \begin{equation}
                        p(f, a \mid e) = \frac{\epsilon}{(l+1)^m}\mathlarger{\prod\limits_{j=1}^{m}t(f_{j}|e_{a(j)})}
                \end{equation}

        The strength features in section~\ref{sec:strength} assign a phrase-level score to a given translation pair. The score does not reflect the actual many-to-one alignments between the phrases. A Model 1 score learns the likelihood of the alignment of the individual words, while also considering the fact that a triangulated table will have less number of source phrases translating into good and some noisy translations. Noisy translations will automatically get a lower Model 1 score, something less likely to happen when using the simpler approach of multiplying the lexical scores. This effect of noisy translations ending up as a viable translation during decoding is also because of the limited source pivot training corpora available. Several translations have been only seen once and the phrase lengths are not very long either ( 90\% of Mawukakan and Maninkakan phrase table has less than or equal to 3 words). A modified model 1 score is also used in~\cite{Cohn:07} in the absence of word alignments. They report a BLEU score improvement of 2 points over the standard feature set when using the Model 1 score, but we observe a different pattern altogether across all the four resource-poor languages~\ref{sec:results}.

\subsection{Joint and Conditional Distributions}
\label{subsec:joint}

Another way of calculating the triangulated phrase scores $p_{tr}(e \mid f)$ and $p_{tr}(f \mid e)$ would be to take the joint probability $p_{tr}(s, t)$ and decompose it to get the conditional distributions. But, we do not have the counts in the triangulated phrase table. The pairs that end up in the triangulated table are contingent on the common pivot phrases that connected the source target pair, thus, counting the pairs after triangulation will not be a true reflection of the joint probability. For a triangulated table between \emph{src} and \emph{tgt}, using source pivot table \emph{sp} and pivot target table \emph{pt}, we can compute the joint probability of a phrase pair (s, t) as follows:

        \begin{eqnarray*}
                p_{tr}(s, t) &=& \sum_{i}p_{sp}(s, i) p_{pt}(i, t) \\
                                &=& \sum_{i}p_{sp}(s \mid i) p_{sp}(i) p_{pt}(i \mid t) p_{pt}(t)
        \end{eqnarray*}


This is a more accurate description of the joint probability of the (s, t) phrase pair in the triangulated table because we are using source pivot and pivot target counts, both of which have been extracted from the alignments. As we have the counts for the direct system, computing the joint and the conditional distributions is relatively straight-forward. When interpolating the triangulated and direct translation models~\eqref{eq:interpolation}, the three new features are added to the log-linear pipeline. Owing to the smaller size of the source pivot corpora, we observed it was better to add the three new features to the log-linear pipeline and let MERT decide which features lead to a better BLEU score. This is in contrast to~\cite{Cohn:07} where they combine the joint probability of the phrase pair from direct and triangulated uniformly, and use the resulting conditionals as part of the log-linear pipeline.


\section{Translation Model Combination}
\label{sec:interpolation}
        When combining two or more translation models, one needs relevant weights assigned to the features from the various models with the aim of maximizing the BLEU score on a heldout set. When the models are from different domains, we want to find the best parameters such that the best translation wins for each given source phrase. For instance, consider the word ``tranblemannt\`e''. It gets translated to \emph{shaking} by our best baseline system. After interpolating our top-20 triangulated translation model, it gets translated to earthquake. Note that the word earthquake is present in the baseline translation model but does not end up as a translation for the source word ``tranblemannt\`e".

        In our experiments, we have a direct translation model and a triangulated translation model from source to target. To learn the best weight $\lambda_{d}$ for the direct system, we perform grid search over the possible co-efficients to find the best $\lambda_{d}$.


        \begin{equation} \label{eq:interpolation}
                p_{interp}(s \mid t) = \lambda_{d} p_{d}(s \mid t) + (1 - \lambda_{d}) p_{t}(s \mid t)
        \end{equation}

        Given a source target phrase pair (s, t), we use uniform linear interpolation as shown in~\eqref{eq:interpolation} to scale all the feature values. In our experiments, we found that epsilon smoothing when the phrase pair is found in only one distribution had no effect. For all the results mentioned, no smoothing is performed.

         \begin{algorithm}
                \small
                \caption{Grid Search for Interpolation}
                \label{algo:condor}
                \textbf{Input:} triangulated phrase table p$_{t}$, \\ direct phrase table p$_{d}$, \\
                $\lambda_{d}$, $\lambda_{t} = 1 - \lambda_{d}$, prev$_{bleu}$ = 0, \\
                minimum = $e ^{-2}$ \\
                \textbf{Output:} best$_{\lambda_{d}}$


                \begin{algorithmic}[l]
                        \WHILE{$\delta_{bleu}$ $>$ minimum} \label{aline:condition}
                        \STATE{interpolate p$_{d}$, p$_{t}$ to give p$_{interp}$} \label{aline:inter}
                        %\STATE{p$_{interp}$  = $\lambda_{d}$$p_{d}$ + $\lambda_{t}$$p_{t}$}
                        \STATE{Run MERT using p$_{interp}$ as translation model}
                        \STATE{find bleu$_{heldout}$}
                        \STATE{$\delta_{bleu}$ = bleu$_{heldout}$ - prev$_{bleu}$}
                        \STATE{prev$_{bleu}$ = bleu$_{heldout}$}
                        \STATE{Based on $\delta_{bleu}$, find new$\lambda_{d}$} \label{aline:search}
                        \ENDWHILE
                       \end{algorithmic}
        \end{algorithm}

       Algorithm~\ref{algo:condor} explains the process of using CONDOR~\cite{condor-practice} to find the best interpolation co-efficient for a given direct and triangulated model. Note that the approach can be easily extended to multiple triangulated models. Line~\ref{aline:inter} interpolates the two translation models using equation~\eqref{eq:interpolation}. We re-tune the log-linear weights using MERT for the interpolated feature values and use the tuned model to find BLEU score on the heldout set. Based on the difference between the BLEU score obtained and the previous BLEU(line~\ref{aline:search}), CONDOR searches for the new co-efficient in the corresponding direction. The search will culminate when consecutive BLEU scores show a marginal difference~\ref{aline:condition}. As can be seen in the figure ~\ref{fig:condor}, we started with a value of 0.85 for the direct system from Mawu to English. This got a BLEU score of 10.21. If we had taken a heuristic approach, we would have just taken 0.50 and 0.50, and stopped. Alternatively, we might have tried 0.50, 0.60, 0.70 and 0.80~\cite{Nakov:12}, keeping in mind the different domains. But, neither strategy would have helped us reach our best BLEU score of 10.91, observed with $\lambda_{d}$ = 0.612962. If we use uniform weights for both the tables, we get BLEU scores on heldout as shown in Table~\ref{table:all_results}. In three of four cases, we would not have out-performed our baseline.


\section{Simulating low-resource}
\label{sec:simulating}

	~\cite{Cohn:07} simulated a low-resource scenario by considering the top 10K sentences from each of the Europarl languages. Having discussed the triangulation in Algorithm~\ref{algo:vanilla_triangulation}, we realize that using triangulation for  a large corpora~\cite{Utiyama:07,Nakov:12,Wu:11,Nakovemnlp:12,Gispert:06} leads to results that do not translate to low-resource languages. The question that comes up is if the setting in~\cite{Cohn:07} is realistic enough ? 

	\alert{Add OOV numbers and say its not}

\section{Vanilla Triangulation}

\begin{table*}
	\small
	\centering
	\input{files/Tables/eparl-100k.tex}
	\caption{Baselines for fr, de, es}
	\label{table:eparlbaselines}
\end{table*}

Using 100K multi-parallel sentences as our corpus for the direct system, we get BLEU scores as reported in Table ~\ref{table:eparlbaselines}. 

 Before we discuss the various design choices in triangulation, BLEU scores when using triangulation as described in Algorithm~\ref{algo:vanilla_triangulation} are reported in Table~\ref{table:eparltopn}. Observe the consistent drop in BLEU scores when compared to the baseline. The results disagree with those reported in~\cite{Utiyama:07,Cohn:07}. Both were able to achieve comparable performance to the direct system as the data was multi-parallel and the same size for all the constituent systems. 

 \begin{table*}[ht]
 	\small
 	\centering
	\input{files/Tables/eparl-topn.tex} 
	\caption{BLEU scores when only using the triangulated translation model}
	\label{table:eparltopn}
 \end{table*}

